import streamlit as st
import cv2
import numpy as np
import base64
from openai import OpenAI
import tempfile
import os
from PIL import Image
import io
import json
# ç¡®ä¿å®‰è£…çš„æ˜¯ moviepy==1.0.3
from moviepy.editor import VideoFileClip

# --- 1. é…ç½®ä¸å¯†é’¥åŠ è½½ ---
st.set_page_config(
    page_title="è§†å¬è¯­è¨€åˆ†æå·¥ä½œç«™", 
    page_icon="ğŸ¬", 
    layout="centered", 
    initial_sidebar_state="collapsed"
)

try:
    VISION_API_KEY = st.secrets["vision"]["api_key"]
    VISION_BASE_URL = st.secrets["vision"]["base_url"]
    VISION_MODEL = st.secrets["vision"]["model"]
    
    AUDIO_API_KEY = st.secrets["audio"]["api_key"]
    AUDIO_BASE_URL = st.secrets["audio"]["base_url"]
    AUDIO_MODEL = st.secrets["audio"]["model"]
except Exception as e:
    st.error(f"âš ï¸ é…ç½®ç¼ºå¤±: {e}ã€‚è¯·æ£€æŸ¥ secrets.toml")
    st.stop()

# --- 2. é¡¶çº§ UI è®¾è®¡ (å¤åˆ»å‚è€ƒå›¾é£æ ¼) ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700;900&display=swap');

    /* === å…¨å±€æ·±è‰²ä¸»é¢˜é‡ç½® === */
    .stApp {
        background-color: #0B0E14; /* æ·±é‚ƒé»‘è“èƒŒæ™¯ */
        font-family: 'Noto Sans SC', sans-serif;
    }
    
    /* å¼ºåˆ¶æ‰€æœ‰æ–‡å­—é¢œè‰²ï¼Œè§£å†³çœ‹ä¸æ¸…çš„é—®é¢˜ */
    h1, h2, h3, p, div, span, label {
        color: #FFFFFF !important;
    }
    .stMarkdown p {
        color: #B0B6BE !important; /* æ­£æ–‡ç¨å¾®ç°ä¸€ç‚¹ï¼Œå½¢æˆå±‚æ¬¡ */
    }

    /* === æ ‡é¢˜åŒºåŸŸ === */
    h1 {
        font-size: 2.8rem !important;
        font-weight: 900 !important;
        text-align: center;
        margin-top: 20px;
        margin-bottom: 10px;
        letter-spacing: 2px;
        text-shadow: 0 0 20px rgba(41, 121, 255, 0.3); /* è“è‰²å¾®å…‰ */
    }
    .subtitle {
        text-align: center;
        color: #8E95A3 !important;
        font-size: 1rem;
        margin-bottom: 40px;
        font-weight: 400;
    }

    /* === Tab å¯¼èˆªæ  (å¤åˆ»èƒ¶å›Šé£æ ¼) === */
    /* å®¹å™¨è°ƒæ•´ï¼šå»é™¤åº•çº¿ï¼Œå±…ä¸­ */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background-color: transparent;
        border-bottom: none !important;
        display: flex;
        flex-wrap: nowrap; /* ç¦æ­¢æ¢è¡Œ */
        white-space: nowrap;
        margin-bottom: 30px;
    }
    
    /* å•ä¸ª Tab æŒ‰é’® (æœªé€‰ä¸­çŠ¶æ€) */
    .stTabs [data-baseweb="tab"] {
        height: 44px;
        border-radius: 22px; /* èƒ¶å›Šåœ†è§’ */
        background-color: #1E232E; /* æ·±ç°åº•è‰² */
        color: #B0B6BE !important;
        border: 1px solid #2D3342;
        font-size: 14px;
        font-weight: 500;
        padding: 0 16px; /* å‹ç¼©å†…è¾¹è·ï¼Œé˜²æ­¢æº¢å‡º */
        flex-grow: 1; /* è‡ªåŠ¨æ’‘æ»¡å®½åº¦ */
        justify-content: center;
        transition: all 0.2s;
    }
    
    /* é¼ æ ‡æ‚¬åœ */
    .stTabs [data-baseweb="tab"]:hover {
        background-color: #2D3342;
        color: #FFFFFF !important;
    }

    /* é€‰ä¸­çŠ¶æ€ (é«˜äº®è“) */
    .stTabs [aria-selected="true"] {
        background-color: #2979FF !important; /* å‚è€ƒå›¾çš„äº®è“ */
        color: #FFFFFF !important;
        border: none;
        box-shadow: 0 4px 15px rgba(41, 121, 255, 0.4); /* å‘å…‰æ•ˆæœ */
    }

    /* === ä¸Šä¼ æ¡†ç¾åŒ– === */
    [data-testid='stFileUploader'] {
        background-color: rgba(30, 35, 46, 0.6);
        border: 2px dashed #444C5C;
        border-radius: 20px;
        padding: 40px 20px;
        text-align: center;
        transition: all 0.3s;
    }
    [data-testid='stFileUploader']:hover {
        border-color: #2979FF;
        background-color: rgba(41, 121, 255, 0.05);
    }
    [data-testid='stFileUploader'] section { background-color: transparent !important; }
    /* éšè—å¤šä½™å°å­— */
    [data-testid='stFileUploader'] small { display: none; }

    /* === æŒ‰é’®æ ·å¼ === */
    .stButton > button {
        background: linear-gradient(135deg, #2979FF, #1565C0);
        color: white !important;
        border: none;
        border-radius: 12px;
        padding: 12px 0;
        font-weight: 700;
        font-size: 16px;
        box-shadow: 0 4px 10px rgba(0,0,0,0.3);
        margin-top: 10px;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 15px rgba(41, 121, 255, 0.4);
    }

    /* === ç»“æœå¡ç‰‡ç³»ç»Ÿ === */
    .info-card {
        background-color: #161920;
        border-radius: 16px;
        padding: 24px;
        margin-bottom: 20px;
        border: 1px solid #2A2F3A;
        position: relative;
        overflow: hidden;
    }
    
    /* è£…é¥°æ€§å·¦è¾¹æ¡† */
    .card-style { border-left: 6px solid #FF4081; }
    .card-shot  { border-left: 6px solid #FFD740; }
    .card-prompt{ border-left: 6px solid #448AFF; }
    .card-audio { border-left: 6px solid #00E676; }
    .card-ocr   { border-left: 6px solid #FF6E40; }

    /* å¡ç‰‡æ ‡é¢˜ */
    .card-header {
        font-size: 1.1rem;
        font-weight: 700;
        margin-bottom: 12px;
        display: flex;
        align-items: center;
        gap: 8px;
    }
    
    /* é¢œè‰²å®šä¹‰ */
    .pink { color: #FF4081 !important; }
    .yellow { color: #FFD740 !important; }
    .blue { color: #448AFF !important; }
    .green { color: #00E676 !important; }
    .orange { color: #FF6E40 !important; }

    /* å†…å®¹æ–‡æœ¬ */
    .card-content {
        font-family: 'JetBrains Mono', monospace;
        font-size: 0.95rem;
        line-height: 1.7;
        color: #D1D5DB !important;
        background: rgba(255,255,255,0.03);
        padding: 12px;
        border-radius: 8px;
    }

    /* å›¾ç‰‡å®¹å™¨åœ†è§’ */
    img { border-radius: 12px; }

</style>
""", unsafe_allow_html=True)

# --- 3. é€»è¾‘å‡½æ•° (ä¿æŒåŠŸèƒ½ä¸å˜) ---

def get_image_base64(image_array):
    img = Image.fromarray(cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB))
    buffer = io.BytesIO()
    img.save(buffer, format="JPEG")
    return base64.b64encode(buffer.getvalue()).decode('utf-8')

def get_frame_at_time(video_path, time_sec=1.5):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0: fps = 30.0
    frame_id = int(fps * time_sec)
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)
    ret, frame = cap.read()
    cap.release()
    return frame if ret else None

def detect_scenes_ignore_subtitles(video_path, threshold=30.0):
    cap = cv2.VideoCapture(video_path)
    frames = []
    timestamps = []
    prev_hist = None
    frame_count = 0
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0: fps = 30.0 
    while True:
        ret, frame = cap.read()
        if not ret: break
        if frame_count % 15 == 0: 
            height, width, _ = frame.shape
            crop_h = int(height * 0.8) 
            cropped_frame = frame[0:crop_h, :] 
            hsv = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2HSV)
            hist = cv2.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])
            cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)
            if prev_hist is None:
                frames.append(frame)
                timestamps.append(frame_count / fps)
                prev_hist = hist
            else:
                score = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CORREL)
                if (1 - score) > (threshold / 100.0) and (frame_count / fps - timestamps[-1] > 1.5):
                    frames.append(frame)
                    timestamps.append(frame_count / fps)
                    prev_hist = hist
        frame_count += 1
    cap.release()
    return frames, timestamps

def analyze_image_reverse_engineering(image_base64):
    client = OpenAI(api_key=VISION_API_KEY, base_url=VISION_BASE_URL)
    system_prompt = """
    è¯·åˆ†æå›¾ç‰‡ï¼Œä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼ˆä¸è¦ Markdownï¼‰ï¼š
    {
        "style": "é£æ ¼æç¤ºè¯...",
        "shot": "é•œå¤´ä¸æ™¯åˆ«...",
        "prompt": "è‹±æ–‡ç”Ÿæˆæç¤ºè¯..."
    }
    """
    try:
        response = client.chat.completions.create(
            model=VISION_MODEL,
            messages=[
                {"role": "user", "content": [{"type": "text", "text": system_prompt}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}]}
            ], max_tokens=800,
        )
        content = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except:
        return {"style": "Error", "shot": "Error", "prompt": "Error"}

def analyze_video_frame_dual(image_base64):
    client = OpenAI(api_key=VISION_API_KEY, base_url=VISION_BASE_URL)
    system_prompt = """
    åˆ†æè§†é¢‘å¸§ï¼Œå¿½ç•¥å­—å¹•ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºä¸¤éƒ¨åˆ†å†…å®¹ï¼š
    {
        "cn_desc": "ä¸­æ–‡ç”»é¢æè¿°ï¼ˆåŒ…å«ç¯å¢ƒã€ä¸»ä½“ã€åŠ¨ä½œã€æ°›å›´ï¼‰",
        "en_prompt": "High quality English prompt for Sora/Runway video generation"
    }
    """
    try:
        response = client.chat.completions.create(
            model=VISION_MODEL,
            messages=[
                {"role": "user", "content": [{"type": "text", "text": system_prompt}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}]}
            ], max_tokens=500,
        )
        content = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        return {"cn_desc": "è§£æå¤±è´¥", "en_prompt": str(e)}

def analyze_ocr_text(image_base64):
    client = OpenAI(api_key=VISION_API_KEY, base_url=VISION_BASE_URL)
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ OCR æ–‡å­—è¯†åˆ«åŠ©æ‰‹ã€‚è¯·è¯†åˆ«ç”»é¢ä¸­å‡ºç°çš„æ‰€æœ‰ã€å›ºå®šä¸­æ–‡æ–‡å­—ã€‘ï¼Œå¿½ç•¥åº•éƒ¨çš„å³æ—¶å­—å¹•ã€‚ç›´æ¥è¾“å‡ºå†…å®¹ã€‚"
    try:
        response = client.chat.completions.create(
            model=VISION_MODEL,
            messages=[
                {"role": "user", "content": [{"type": "text", "text": system_prompt}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}]}
            ], max_tokens=500,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"OCR Error: {str(e)}"

def transcribe_audio_api(video_path):
    try:
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as temp_audio:
            audio_path = temp_audio.name
        video = VideoFileClip(video_path)
        video.audio.write_audiofile(audio_path, codec='mp3', logger=None, ffmpeg_params=["-ac", "1"])
        video.close()
        client = OpenAI(api_key=AUDIO_API_KEY, base_url=AUDIO_BASE_URL)
        with open(audio_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format="text")
        os.remove(audio_path)
        if isinstance(transcript, str):
            try:
                data = json.loads(transcript)
                if "text" in data: return data["text"]
            except: pass
            return transcript
        return transcript.text
    except Exception as e:
        return f"Audio Error: {str(e)}"

# --- 4. ç•Œé¢æ¸²æŸ“ ---

# æ ‡é¢˜åŒº
st.markdown("<h1>è§†å¬è¯­è¨€åˆ†æå·¥ä½œç«™</h1>", unsafe_allow_html=True)
st.markdown("<div class='subtitle'>Visual Intelligence Analysis Workstation</div>", unsafe_allow_html=True)

# Tab å¯¼èˆªåŒº (åç§°ç®€åŒ–ï¼Œé˜²æ­¢æº¢å‡º)
tab1, tab2, tab3, tab4 = st.tabs(["å›¾ç”Ÿæ–‡åæ¨", "è§†é¢‘æ‹†è§£", "å£æ’­æ‰’å–", "æ–‡å­—æå–"])

# === Tab 1: å›¾ç”Ÿæ–‡ ===
with tab1:
    st.markdown("<div style='text-align:center; color:#888; margin-bottom:10px;'>ä¸Šä¼ å‚è€ƒå›¾ç‰‡ï¼ŒAI å°†åˆ†åˆ«åæ¨å…¶é£æ ¼ã€é•œå¤´è¯­è¨€åŠå®Œæ•´çš„ç”Ÿå›¾æç¤ºè¯ã€‚</div>", unsafe_allow_html=True)
    
    # å±…ä¸­å¸ƒå±€
    c1, c2, c3 = st.columns([1, 2, 1])
    with c2:
        uploaded_img = st.file_uploader(" ", type=["jpg", "png"], key="img_up")

    if uploaded_img:
        st.write("")
        c_disp, c_a
