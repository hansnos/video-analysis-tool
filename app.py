import streamlit as st
import cv2
import numpy as np
import base64
from openai import OpenAI
import tempfile
import os
from PIL import Image
import io
import json
# ç¡®ä¿å®‰è£…çš„æ˜¯ moviepy==1.0.3
from moviepy.editor import VideoFileClip

# --- 1. é…ç½®ä¸å¯†é’¥åŠ è½½ ---
st.set_page_config(
    page_title="è§†å¬è¯­è¨€åˆ†æå·¥ä½œç«™", 
    page_icon="ğŸ¬", 
    layout="wide", # <--- æ”¹ä¸º wideï¼Œä¸ºäº†è®©ç»“æœå±•ç¤ºæ›´å®½é˜”
    initial_sidebar_state="collapsed"
)

try:
    VISION_API_KEY = st.secrets["vision"]["api_key"]
    VISION_BASE_URL = st.secrets["vision"]["base_url"]
    VISION_MODEL = st.secrets["vision"]["model"]
    
    AUDIO_API_KEY = st.secrets["audio"]["api_key"]
    AUDIO_BASE_URL = st.secrets["audio"]["base_url"]
    AUDIO_MODEL = st.secrets["audio"]["model"]
except Exception as e:
    st.error(f"âš ï¸ é…ç½®ç¼ºå¤±: {e}ã€‚è¯·æ£€æŸ¥ secrets.toml")
    st.stop()

# --- 2. æ ·å¼å¾®è°ƒ (é€‚é… Wide æ¨¡å¼ä½†ä¿æŒè¾“å…¥å±…ä¸­) ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700;900&display=swap');

    .stApp { background-color: #0B0E14; font-family: 'Noto Sans SC', sans-serif; }
    h1, h2, h3, p, div, span, label { color: #FFFFFF !important; }
    .stMarkdown p { color: #B0B6BE !important; }

    /* æ ‡é¢˜ */
    h1 {
        font-size: 2.8rem !important; font-weight: 900 !important; text-align: center;
        margin-top: 20px; margin-bottom: 10px; letter-spacing: 2px;
        text-shadow: 0 0 20px rgba(41, 121, 255, 0.3);
    }
    .subtitle { text-align: center; color: #8E95A3 !important; font-size: 1rem; margin-bottom: 40px; }

    /* Tab å¯¼èˆªæ  (å±…ä¸­) */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px; background-color: transparent; border-bottom: none !important;
        display: flex; justify-content: center; /* å¼ºåˆ¶å±…ä¸­ */
        flex-wrap: nowrap; margin-bottom: 30px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 44px; border-radius: 22px; background-color: #1E232E; color: #B0B6BE !important;
        border: 1px solid #2D3342; font-size: 14px; font-weight: 500; padding: 0 30px;
        transition: all 0.2s;
    }
    .stTabs [data-baseweb="tab"]:hover { background-color: #2D3342; color: #FFFFFF !important; }
    .stTabs [aria-selected="true"] {
        background-color: #2979FF !important; color: #FFFFFF !important; border: none;
        box-shadow: 0 4px 15px rgba(41, 121, 255, 0.4);
    }

    /* ä¸Šä¼ æ¡† (é»‘åº•è“æ¡†) */
    [data-testid='stFileUploader'] {
        background-color: rgba(30, 35, 46, 0.6); border: 2px dashed #444C5C; border-radius: 20px;
        padding: 40px 20px; text-align: center; transition: all 0.3s;
    }
    [data-testid='stFileUploader']:hover { border-color: #2979FF; background-color: rgba(41, 121, 255, 0.05); }
    [data-testid='stFileUploader'] section { background-color: transparent !important; }
    [data-testid='stFileUploader'] small { display: none; }

    /* ç»“æœå¡ç‰‡ */
    .info-card {
        background-color: #161920; border-radius: 16px; padding: 20px; margin-bottom: 20px;
        border: 1px solid #2A2F3A; position: relative;
    }
    .card-style { border-left: 6px solid #FF4081; }
    .card-shot  { border-left: 6px solid #FFD740; }
    .card-prompt{ border-left: 6px solid #448AFF; }
    .card-audio { border-left: 6px solid #00E676; }
    .card-ocr   { border-left: 6px solid #FF6E40; }
    .card-cn    { border-left: 6px solid #9C27B0; }

    .card-header { font-size: 1.1rem; font-weight: 700; margin-bottom: 12px; display: flex; align-items: center; gap: 8px; }
    .pink { color: #FF4081 !important; }
    .yellow { color: #FFD740 !important; }
    .blue { color: #448AFF !important; }
    .green { color: #00E676 !important; }
    .orange { color: #FF6E40 !important; }
    .purple { color: #9C27B0 !important; }

    .card-content {
        font-family: 'JetBrains Mono', monospace; font-size: 1rem; line-height: 1.6;
        color: #D1D5DB !important; background: rgba(255,255,255,0.03); padding: 12px; border-radius: 8px;
    }
    img { border-radius: 12px; }
    
    /* ä¸‹è½½æŒ‰é’®ç¾åŒ– */
    .stDownloadButton button {
        background-color: transparent !important;
        border: 1px solid #444 !important;
        color: #888 !important;
        font-size: 12px;
        padding: 5px 15px;
    }
    .stDownloadButton button:hover {
        border-color: #2979FF !important;
        color: #2979FF !important;
    }
</style>
""", unsafe_allow_html=True)

# --- 3. æ ¸å¿ƒé€»è¾‘å‡½æ•° ---

def get_image_base64(image_array):
    img = Image.fromarray(cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB))
    buffer = io.BytesIO()
    img.save(buffer, format="JPEG")
    return base64.b64encode(buffer.getvalue()).decode('utf-8')

# æ–°å¢ï¼šç”¨äºä¸‹è½½å›¾ç‰‡çš„è½¬æ¢å‡½æ•°
def convert_frame_to_bytes(frame_array):
    # OpenCV BGR -> RGB -> Bytes
    img = Image.fromarray(cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB))
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()

def get_frame_at_time(video_path, time_sec=1.5):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0: fps = 30.0
    frame_id = int(fps * time_sec)
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)
    ret, frame = cap.read()
    cap.release()
    return frame if ret else None

def detect_scenes_ignore_subtitles(video_path, threshold=30.0):
    cap = cv2.VideoCapture(video_path)
    frames = []
    timestamps = []
    prev_hist = None
    frame_count = 0
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0: fps = 30.0 
    while True:
        ret, frame = cap.read()
        if not ret: break
        if frame_count % 15 == 0: 
            height, width, _ = frame.shape
            crop_h = int(height * 0.8) 
            cropped_frame = frame[0:crop_h, :] 
            hsv = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2HSV)
            hist = cv2.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])
            cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)
            if prev_hist is None:
                frames.append(frame)
                timestamps.append(frame_count / fps)
                prev_hist = hist
            else:
                score = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CORREL)
                if (1 - score) > (threshold / 100.0) and (frame_count / fps - timestamps[-1] > 1.5):
                    frames.append(frame)
                    timestamps.append(frame_count / fps)
                    prev_hist = hist
        frame_count += 1
    cap.release()
    return frames, timestamps

def analyze_image_reverse_engineering(image_base64):
    """
    å›¾ç”Ÿæ–‡åæ¨æ¨¡å¼ï¼šå‡çº§ç‰ˆ System Promptï¼Œè¿½æ±‚ 95% è¿˜åŸåº¦
    """
    client = OpenAI(api_key=VISION_API_KEY, base_url=VISION_BASE_URL)
    
    # === æ ¸å¿ƒä¿®æ”¹ï¼šèµ‹äºˆ AI ä¸“å®¶äººè®¾ï¼Œè¦æ±‚æåº¦ç²¾å‡†çš„å…³é”®è¯ ===
    system_prompt = """
    ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ AI ç»˜ç”»æç¤ºè¯å·¥ç¨‹å¸ˆï¼ˆPrompt Engineerï¼‰ï¼Œç²¾é€š Midjourneyã€Stable Diffusion å’Œ Flux çš„æç¤ºè¯é€»è¾‘ã€‚
    è¯·æ·±åº¦å‰–æè¿™å¼ å›¾ç‰‡ï¼Œåæ¨å‡ºèƒ½å®Œç¾è¿˜åŸè¯¥ç”»é¢çš„æç¤ºè¯ã€‚
    
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆä¸è¦ Markdownï¼‰ï¼š
    {
        "style": "è¿™é‡Œåˆ—å‡ºæ ¸å¿ƒè‰ºæœ¯é£æ ¼ã€‚ä¾‹å¦‚ï¼šCyberpunk, Ukiyo-e, Oil Painting, 3D Render (Octane), Pixar Style, Matte Painting...",
        "shot": "è¿™é‡Œåˆ—å‡ºé•œå¤´ä¸å…‰å½±ã€‚ä¾‹å¦‚ï¼šWide angle, Telephoto lens, Dutch angle, Volumetric lighting, Rim light, Bokeh...",
        "prompt": "è¿™é‡Œç¼–å†™ä¸€æ®µé«˜è´¨é‡çš„è‹±æ–‡ Promptã€‚å¿…é¡»åŒ…å«ï¼š
                   1. ä¸»ä½“ç»†èŠ‚ï¼ˆäº”å®˜ã€è¡£ç€æè´¨ã€è¡¨æƒ…ï¼‰ã€‚
                   2. ç¯å¢ƒç»†èŠ‚ï¼ˆèƒŒæ™¯å…ƒç´ ã€å¤©æ°”ï¼‰ã€‚
                   3. æŠ€æœ¯å‚æ•°ï¼ˆå¦‚ï¼š8k, photorealistic, masterpiece, highly detailed, unreal engine 5ï¼‰ã€‚
                   è¯·ä½¿ç”¨é€—å·åˆ†éš”çš„å…³é”®è¯å½¢å¼ã€‚"
    }
    """
    
    try:
        response = client.chat.completions.create(
            model=VISION_MODEL,
            messages=[
                {"role": "user", "content": [
                    {"type": "text", "text": system_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                ]}
            ],
            max_tokens=800,
        )
        content = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        return {"style": "Error", "shot": "Error", "prompt": str(e)}

def analyze_video_frame_reconstruction(image_base64):
    """
    é’ˆå¯¹ 90% è¿˜åŸåº¦çš„ç”»é¢å¸§åæ¨ Prompt (å‡çº§ç‰ˆï¼šå¢å¼ºé£æ ¼ä¸èº«ä»½è¯†åˆ«)
    """
    client = OpenAI(api_key=VISION_API_KEY, base_url=VISION_BASE_URL)
    
    # === è¿™é‡Œæ˜¯æ ¸å¿ƒä¿®æ”¹ï¼šå¤§å¹…å¢å¼ºäº†æç¤ºè¯çš„è¦æ±‚ ===
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„ AI è‰ºæœ¯å¯¼æ¼”å’Œæç¤ºè¯ä¸“å®¶ã€‚
    è¯·æ·±åº¦åˆ†æè¿™å¼ è§†é¢‘æˆªå›¾ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆä¸€æ®µèƒ½è®© Midjourney/Sora å®Œç¾è¿˜åŸç”»é¢ç¥éŸµçš„è‹±æ–‡ Promptã€‚
    
    è¯·ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹ç»†èŠ‚çš„æå–ï¼š
    1. **äººç‰©èº«ä»½ä¸ç‰¹å¾**ï¼šä¸è¦åªè¯´ "Person"ã€‚è¯·ä»”ç»†è§‚å¯Ÿè¡£ç€ï¼ˆå¦‚é•¿è¢ã€æ–—ç¬ ã€ç ´æ—§è¡£ç‰©ï¼‰ï¼Œåˆ¤æ–­æ˜¯å¦ä¸º Monk (åƒ§äºº), Daoist (é“å£«), Wanderer (æµæµªè€…) æˆ– Elder (è€è€…)ã€‚
    2. **æ‘„å½±ä¸è‰ºæœ¯é£æ ¼**ï¼šè¿™æ˜¯å†™å®ç…§ç‰‡ã€CGæ¸²æŸ“è¿˜æ˜¯é»‘ç™½ç”µå½±ï¼Ÿå¦‚æœæ˜¯é»‘ç™½çš„ï¼Œè¯·åŠ ä¸Š "Black and white photography, vintage style, film grain" ç­‰å…³é”®è¯ã€‚
    3. **ç¯å¢ƒä¸æ°›å›´**ï¼šæè¿°å¤©æ°”ï¼ˆé˜´æ²‰ã€è¿·é›¾ï¼‰ã€å…‰å½±ï¼ˆæŸ”å…‰ã€é€†å…‰ï¼‰åŠç”»é¢çš„æƒ…ç»ªï¼ˆå­¤ç‹¬ã€å²è¯—æ„Ÿï¼‰ã€‚
    
    è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºï¼š
    {
        "cn_desc": "ä¸­æ–‡æ·±åº¦ç”»é¢æè¿°ï¼ˆå¿…é¡»æ˜ç¡®å†™å‡ºäººç‰©èº«ä»½ï¼Œå¦‚ï¼šèƒŒè´Ÿè¡Œå›Šçš„è‹¦è¡Œåƒ§/è€é“å£«ï¼Œä»¥åŠç”»é¢çš„é»‘ç™½å¤å¤è´¨æ„Ÿï¼‰",
        "en_prompt": "High-fidelity English text-to-image prompt. Include keywords for: Subject Identity (e.g., old monk, ascetic), Clothing (traditional robes), Art Style (e.g., 1920s vintage photography, black and white, grainy film), Lighting, and Atmosphere."
    }
    ä¸è¦è¾“å‡º Markdown æ ‡è®°ã€‚
    """
    
    try:
        response = client.chat.completions.create(
            model=VISION_MODEL,
            messages=[
                {"role": "user", "content": [
                    {"type": "text", "text": system_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                ]}
            ],
            max_tokens=800, # ç¨å¾®å¢åŠ äº† token é™åˆ¶ä»¥å…è®¸æ›´è¯¦ç»†çš„æè¿°
        )
        content = response.choices[0].message.content.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        return {"cn_desc": "è§£æå¤±è´¥", "en_prompt": str(e)}

def analyze_ocr_text(image_base64):
    client = OpenAI(api_key=VISION_API_KEY, base_url=VISION_BASE_URL)
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ OCR æ–‡å­—è¯†åˆ«åŠ©æ‰‹ã€‚è¯·è¯†åˆ«ç”»é¢ä¸­å‡ºç°çš„æ‰€æœ‰ã€å›ºå®šä¸­æ–‡æ–‡å­—ã€‘ï¼Œå¿½ç•¥åº•éƒ¨çš„å³æ—¶å­—å¹•ã€‚ç›´æ¥è¾“å‡ºå†…å®¹ã€‚"
    try:
        response = client.chat.completions.create(
            model=VISION_MODEL,
            messages=[
                {"role": "user", "content": [{"type": "text", "text": system_prompt}, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}]}
            ], max_tokens=500,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"OCR Error: {str(e)}"

def transcribe_audio_api(video_path):
    try:
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as temp_audio:
            audio_path = temp_audio.name
        video = VideoFileClip(video_path)
        video.audio.write_audiofile(audio_path, codec='mp3', logger=None, ffmpeg_params=["-ac", "1"])
        video.close()
        client = OpenAI(api_key=AUDIO_API_KEY, base_url=AUDIO_BASE_URL)
        with open(audio_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format="text")
        os.remove(audio_path)
        if isinstance(transcript, str):
            try:
                data = json.loads(transcript)
                if "text" in data: return data["text"]
            except: pass
            return transcript
        return transcript.text
    except Exception as e:
        return f"Audio Error: {str(e)}"

# --- 4. ç•Œé¢æ¸²æŸ“ ---

st.markdown("<h1>è§†å¬è¯­è¨€åˆ†æå·¥ä½œç«™</h1>", unsafe_allow_html=True)
st.markdown("<div class='subtitle'>Visual Intelligence Analysis Workstation</div>", unsafe_allow_html=True)

# Tab å¯¼èˆªåŒº
tab1, tab2, tab3, tab4 = st.tabs(["å›¾ç”Ÿæ–‡åæ¨", "è§†é¢‘æ‹†è§£", "å£æ’­æ‰’å–", "æ–‡å­—æå–"])

# === Tab 1: å›¾ç”Ÿæ–‡ ===
with tab1:
    st.markdown("<div style='text-align:center; color:#888; margin-bottom:10px;'>AI åæ¨é£æ ¼ã€é•œå¤´è¯­è¨€åŠç”Ÿå›¾æç¤ºè¯</div>", unsafe_allow_html=True)
    
    # è¾“å…¥åŒºåŸŸå±…ä¸­ (1:2:1 å¸ƒå±€)
    c1, c2, c3 = st.columns([1, 2, 1])
    with c2:
        uploaded_img = st.file_uploader(" ", type=["jpg", "png"], key="img_up")

    if uploaded_img:
        # è‡ªåŠ¨åŒ–å¤„ç†ï¼šä¸éœ€è¦æŒ‰é’®ï¼Œç›´æ¥å¼€å§‹
        with st.spinner("AI è§†è§‰å¼•æ“æ­£åœ¨è§£æ..."):
            image = Image.open(uploaded_img)
            buffered = io.BytesIO()
            image.save(buffered, format="JPEG")
            img_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            result = analyze_image_reverse_engineering(img_b64)
            
            # ç»“æœå±•ç¤ºï¼šå·¦å›¾å³æ–‡å¸ƒå±€ (1:2)
            st.write("")
            r1, r2 = st.columns([1, 2])
            with r1:
                st.image(uploaded_img, caption="åŸå§‹å›¾ç‰‡", use_container_width=True)
            with r2:
                st.markdown(f"""
                <div class="info-card card-style">
                    <div class="card-header pink">ğŸ¨ é£æ ¼æç¤ºè¯ (Style)</div>
                    <div class="card-content">{result.get('style', 'N/A')}</div>
                </div>
                <div class="info-card card-shot">
                    <div class="card-header yellow">ğŸ“· é•œå¤´ä¸æ™¯åˆ« (Shot)</div>
                    <div class="card-content">{result.get('shot', 'N/A')}</div>
                </div>
                <div class="info-card card-prompt">
                    <div class="card-header blue">âœ¨ AI ç”Ÿå›¾æç¤ºè¯ (Prompt)</div>
                    <div class="card-content" style="user-select: all;">{result.get('prompt', 'N/A')}</div>
                </div>
                """, unsafe_allow_html=True)

# === Tab 2: è§†é¢‘æ‹†è§£ (æ ¸å¿ƒä¿®æ”¹åŒº) ===
with tab2:
    st.markdown("<div style='text-align:center; color:#888; margin-bottom:10px;'>ç”Ÿæˆç”»é¢å¸§åŒè¯­æç¤ºè¯ (é€‚ç”¨äºå³æ¢¦/NanoBananaç”»é¢è¿˜åŸ)</div>", unsafe_allow_html=True)
    
    # è¾“å…¥åŒºåŸŸå±…ä¸­
    t2_c1, t2_c2, t2_c3 = st.columns([1, 2, 1])
    with t2_c2:
        v_file = st.file_uploader(" ", type=["mp4", "mov"], key="v_up")
        threshold = st.slider("åˆ‡é•œçµæ•åº¦", 10, 60, 25)

    if v_file:
        # è‡ªåŠ¨åŒ–å¤„ç†
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(v_file.read())
        
        with st.status("æ­£åœ¨é€å¸§åˆ†æä¸ç”Ÿæˆæç¤ºè¯...", expanded=True) as status:
            frames, tstamps = detect_scenes_ignore_subtitles(tfile.name, threshold)
            st.write(f"æ£€æµ‹åˆ° {len(frames)} ä¸ªå…³é”®é•œå¤´ï¼Œæ­£åœ¨ç”Ÿæˆè¿˜åŸ Prompt...")
            
            res_container = st.container()
            for i, (frm, ts) in enumerate(zip(frames, tstamps)):
                b64 = get_image_base64(frm)
                res = analyze_video_frame_reconstruction(b64)
                
                with res_container:
                    # ç»“æœå¸ƒå±€ï¼šå›¾ç‰‡å˜å¤§ (2:3 å¸ƒå±€)
                    res_c1, res_c2 = st.columns([2, 3])
                    
                    with res_c1:
                        st.image(frm, channels="BGR", use_container_width=True)
                        # ä¸‹è½½æŒ‰é’®é€»è¾‘
                        img_bytes = convert_frame_to_bytes(frm)
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½è¯¥å¸§",
                            data=img_bytes,
                            file_name=f"frame_{ts:.2f}.png",
                            mime="image/png",
                            key=f"dl_{ts}"
                        )
                        st.caption(f"â±ï¸ æ—¶é—´ç‚¹: {ts:.2f}s")
                        
                    with res_c2:
                        st.markdown(f"""
                        <div class="info-card card-cn" style="margin-bottom:10px;">
                            <div class="card-header purple">ğŸ“ ä¸­æ–‡ç”»é¢æè¿°</div>
                            <div class="card-content">{res.get('cn_desc', '...')}</div>
                        </div>
                        <div class="info-card card-prompt">
                            <div class="card-header blue">âœ¨ ç”»é¢è¿˜åŸ Prompt (Image Gen)</div>
                            <div class="card-content" style="user-select: all;">{res.get('en_prompt', '...')}</div>
                        </div>
                        """, unsafe_allow_html=True)
                    st.divider()
            status.update(label="âœ… åˆ†æå®Œæˆ", state="complete", expanded=False)

# === Tab 3: å£æ’­æ‰’å– ===
with tab3:
    st.markdown("<div style='text-align:center; color:#888; margin-bottom:10px;'>æå–è¯­éŸ³ï¼Œè½¬æ¢ä¸ºé€å­—ç¨¿</div>", unsafe_allow_html=True)
    
    t3_c1, t3_c2, t3_c3 = st.columns([1, 2, 1])
    with t3_c2:
        a_file = st.file_uploader(" ", type=["mp4", "mp3", "wav"], key="a_up")
    
    if a_file:
        # è‡ªåŠ¨åŒ–å¤„ç†
        tfile_a = tempfile.NamedTemporaryFile(delete=False)
        tfile_a.write(a_file.read())
        with st.spinner("AI å¬å†™ä¸­..."):
            txt = transcribe_audio_api(tfile_a.name)
            
            # ç»“æœå±•ç¤ºå±…ä¸­
            r3_c1, r3_c2, r3_c3 = st.columns([1, 6, 1])
            with r3_c2:
                st.audio(a_file)
                st.markdown(f"""
                <div class="info-card card-audio">
                    <div class="card-header green">ğŸ™ï¸ é€å­—ç¨¿ (Transcript)</div>
                    <div class="card-content" style="user-select: all;">{txt}</div>
                </div>
                """, unsafe_allow_html=True)

# === Tab 4: æ–‡å­—æå– ===
with tab4:
    st.markdown("<div style='text-align:center; color:#888; margin-bottom:10px;'>è¯†åˆ«å¤§å­—æŠ¥ã€åŒ…è£…æ–‡å­—åŠå…³é”®ä¿¡æ¯</div>", unsafe_allow_html=True)
    
    t4_c1, t4_c2, t4_c3 = st.columns([1, 2, 1])
    with t4_c2:
        ocr_file = st.file_uploader(" ", type=["mp4", "mov"], key="ocr_up")
    
    if ocr_file:
        # è‡ªåŠ¨åŒ–å¤„ç†
        tfile_ocr = tempfile.NamedTemporaryFile(delete=False)
        tfile_ocr.write(ocr_file.read())
        frame = get_frame_at_time(tfile_ocr.name, time_sec=1.5)
        
        if frame is not None:
            with st.spinner("OCR è¯†åˆ«ä¸­..."):
                b64 = get_image_base64(frame)
                ocr_text = analyze_ocr_text(b64)
                
                # ç»“æœå±•ç¤º (1:1 å¸ƒå±€)
                ocr_c1, ocr_c2 = st.columns([1, 1])
                with ocr_c1:
                    st.image(frame, channels="BGR", caption="è¯†åˆ«å¸§", use_container_width=True)
                with ocr_c2:
                    st.markdown(f"""
                    <div class="info-card card-ocr">
                        <div class="card-header orange">ğŸ”  æå–ç»“æœ (OCR)</div>
                        <div class="card-content" style="white-space: pre-line; user-select: all;">{ocr_text}</div>
                    </div>
                    """, unsafe_allow_html=True)

